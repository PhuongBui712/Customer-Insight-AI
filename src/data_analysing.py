import sys
import re
from tqdm import tqdm
from threading import Lock
from functools import lru_cache
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict
from dotenv import load_dotenv

from utils import *
from prompt import (
    classifying_inquiry_prompt,
    reclassifying_inquiry_prompt,
    extracting_user_purpose_prompt,
    classifying_important_question_prompt,
)
from llm import *


load_dotenv()
LLM_CONFIG = load_yaml(os.path.join(PROJECT_DIRECTORY, 'config.yaml'))['llm']


def keyword_filter(patterns: List[str], messages: List[dict], get_keyword: Optional[bool] = True) -> List[dict]:
    """
    Filter messages based on the presence or absence of specified keywords.

    This function iterates through a list of messages and checks if each message contains any of the given keywords.
    It returns a list of messages that either contain or do not contain the specified keywords, depending on the `get_keyword` flag.

    Args:
        patterns (List[str]): A list of keywords to filter by.
        messages (List[dict]): A list of messages to filter.
        get_keyword (Optional[bool], optional): If True, returns messages containing the keywords. 
            If False, returns messages not containing the keywords. Defaults to True.

    Returns:
        List[dict]: A list of messages that meet the filtering criteria.
    """
    synthetic_pattern = r'\b(' + '|'.join(patterns) + r')\b'
    result = [m for m in messages 
              if bool(re.search(synthetic_pattern, m['message'].lower())) == get_keyword]

    return result


def handle_template_message(templates: Dict[str, Dict[str, str]], messages: List[dict]) -> Tuple[List[dict], List[dict]]:
    """
    Handle template messages.

    This function iterates through a list of messages and checks if each message is a key in the `templates` dictionary.
    If a message is found in the `templates` dictionary, it updates the message with the corresponding template information
    and appends it to the `template_message` list. Otherwise, it appends the message to the `other_message` list.

    Args:
        templates (Dict[str, Dict[str, str]]): A dictionary of template messages, where the key is the message string
            and the value is a dictionary containing the user and purpose information.
        messages (List[dict]): A list of messages to be processed.

    Returns:
        Tuple[List[dict], List[dict]]: A tuple containing two lists:
            - `template_message`: A list of messages that were found in the `templates` dictionary.
            - `other_message`: A list of messages that were not found in the `templates` dictionary.
    """
    template_message = []
    other_message = []
    for m in messages:
        key = m['message'].lower()
        if key in templates:
            m.update(templates[key])
            template_message.append(m)
        else:
            other_message.append(m)

    return template_message, other_message


def call_llm(
        messages: List[str], 
        prompt: ChatPromptTemplate,
        batch_size: int = 50,
        provider: Literal['groq', 'google'] = 'groq',
        desc: Optional[str] = None,
) -> List[Union[dict, bool, float, None]]:
    """
    Calls the specified LLM provider to generate responses for a list of messages.

    This function takes a list of messages, a prompt template, batch size, LLM provider, and an optional description.
    It uses a thread pool executor to process the messages in batches and returns a list of responses.

    Args:
        messages (List[str]): A list of messages to be processed by the LLM.
        prompt (ChatPromptTemplate): A prompt template to be used for generating responses.
        batch_size (int, optional): The number of messages to process in each batch. Defaults to 50.
        provider (Literal['groq', 'google'], optional): The LLM provider to use. Defaults to 'groq'.
        desc (Optional[str], optional): An optional description for the progress bar. Defaults to None.

    Returns:
        List[Union[dict, bool, float, None]]: A parsed list of responses generated by the LLM.
    """
    if provider == "google":
        chain = GoogleAICaller(LLM_CONFIG[provider], prompt)
    else:
        chain = GroqAICaller(LLM_CONFIG[provider], prompt)

    @lru_cache(maxsize=None)
    def cached_invoke(input_str):
        return chain.invoke({"input": input_str})
    
    res = [None for _ in range(len(messages))]
    with ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:
        future = {
            executor.submit(
                lambda: cached_invoke(str(messages[i : i + batch_size]))
            ): i
            for i in range(0, len(messages), batch_size)
        }
        for f in tqdm(as_completed(future), total=len(future), desc=(desc or 'Loading'), file=sys.stdout):
            i = future[f]
            end_idx = min(i + batch_size, len(messages))
            
            try:
                response = f.result()
            except Exception as exc:
                print(f"Error while generating response for batch {i} - {end_idx - 1}")
                continue

            try:
                parsed_response = parse_llm_output(response)
                if len(parsed_response) != end_idx - i:
                    print('Wrong size while query LLM, shutting down!')
                    break

                for j, idx in enumerate(range(i, end_idx)):
                    res[idx] = next(iter(parsed_response[j].values()))
            except Exception:
                print(f"Error while parsing LLM output for batch {i} - {end_idx - 1}")

    return res


def classify_inquiry_pipeline(
    messages: List[dict],
    min_score: float,
    batch_size: int = 50,
    provider: Literal["google", "groq"] = "groq",
) -> Tuple[List[dict], List[dict]]:
    """
    Classifies inquiries based on a minimum score threshold.

    This function takes a list of messages, a minimum score threshold, batch size, and LLM provider.
    It uses the `call_llm` function to generate scores for each message and classifies them based on the threshold.

    Args:
        messages (List[dict]): A list of messages to be classified.
        min_score (float): The minimum score threshold for classifying an inquiry.
        batch_size (int, optional): The number of messages to process in each batch. Defaults to 50.
        provider (Literal["google", "groq"], optional): The LLM provider to use. Defaults to "groq".

    Returns:
        Tuple[List[dict], List[dict]]: A tuple containing two lists:
            - `classified_messages`: A list of messages that meet the minimum score threshold.
            - `error_messages`: A list of messages that do not meet the minimum score threshold.
    """
    input = [m["message"] for m in messages]
    output = call_llm(
        messages=input,
        prompt=classifying_inquiry_prompt,
        batch_size=batch_size,
        provider=provider,
        desc='Classify inquiry'
    )

    classified_messages = []
    error_messages = []
    for message, score in zip(messages, output):
        if score and score >= min_score:
            classified_messages.append(message)
        else:
            error_messages.append(message)

    return classified_messages, error_messages


def reclassify_inquiry_pipeline(
    messages: List[dict],
    batch_size: int = 50,
    provider: Literal["google", "groq"] = "groq",
) -> Tuple[List[dict], List[dict]]:
    """
    Reclassifies inquiries using an LLM.

    This function takes a list of messages, batch size, and LLM provider.
    It uses the `call_llm` function to generate labels for each message and classifies them accordingly.

    Args:
        messages (List[dict]): A list of messages to be reclassified.
        batch_size (int, optional): The number of messages to process in each batch. Defaults to 50.
        provider (Literal["google", "groq"], optional): The LLM provider to use. Defaults to "groq".

    Returns:
        Tuple[List[dict], List[dict]]: A tuple containing two lists:
            - `classified_messages`: A list of messages that were classified as inquiries.
            - `error_messages`: A list of messages that were not classified as inquiries.
    """
    input = [m["message"] for m in messages]
    output = call_llm(
        messages=input,
        prompt=reclassifying_inquiry_prompt,
        batch_size=batch_size,
        provider=provider,
        desc='Re-classify inquiry'
    )

    # get output to return
    classified_messages = []
    error_messages = []
    for message, label in zip(messages, output):
        if label:
            classified_messages.append(message)
        else:
            error_messages.append(message)

    return classified_messages, error_messages


def classify_question_pipeline(
    messages: List[dict],
    batch_size: int = 50,
    provider: Literal["google", "groq"] = "groq",
) -> Tuple[List[dict], List[dict]]:
    """
    Classifies messages as questions using an LLM.

    This function takes a list of messages, batch size, and LLM provider.
    It uses the `call_llm` function to generate labels for each message and classifies them as questions or not.

    Args:
        messages (List[dict]): A list of messages to be classified.
        batch_size (int, optional): The number of messages to process in each batch. Defaults to 50.
        provider (Literal["google", "groq"], optional): The LLM provider to use. Defaults to "groq".

    Returns:
        Tuple[List[dict], List[dict]]: A tuple containing two lists:
            - `classified_messages`: A list of messages that were classified as questions.
            - `error_messages`: A list of messages that were not classified as questions.
    """
    # classify by LLM
    input = [m["message"] for m in messages]
    output = call_llm(
        messages=input,
        prompt=classifying_important_question_prompt,
        batch_size=batch_size,
        provider=provider,
        desc='Classify question'
    )

    # get output to return
    classified_messages = []
    error_messages = []
    for message, label in zip(messages, output):
        if label:
            classified_messages.append(message)
        else:
            error_messages.append(message)

    return classified_messages, error_messages


def extract_user_purpose_pipeline(
    messages: List, 
    batch_size: int = 50, 
    provider: Literal['google', 'groq'] = 'groq'
) -> Tuple[List[dict], List[dict]]:
    """
    Extracts user and purpose information from messages using an LLM.

    This function takes a list of messages, batch size, and LLM provider.
    It uses the `call_llm` function to generate user and purpose information for each message.

    Args:
        messages (List): A list of messages to be processed.
        batch_size (int, optional): The number of messages to process in each batch. Defaults to 50.
        provider (Literal['google', 'groq'], optional): The LLM provider to use. Defaults to 'groq'.

    Returns:
        Tuple[List[dict], List[dict]]: A tuple containing two lists:
            - `extracted_messages`: A list of messages with extracted user and purpose information.
            - `error_messages`: A list of messages where extraction failed.
    """
    # classify by LLM
    input = [m["message"] for m in messages]
    output = call_llm(
        messages=input,
        prompt=extracting_user_purpose_prompt,
        batch_size=batch_size,
        provider=provider,
        desc='Extract inquiry'
    )

    extracted_messages = []
    error_messages = []
    for mess, u_and_p in zip(messages, output):
        if u_and_p and all(u_and_p.values()):
            extracted_message = mess.copy()
            extracted_message.update(u_and_p)
            extracted_messages.append(extracted_message)
        else:
            error_messages.append(mess)
        
    return extracted_messages, error_messages


def analyse_message_pipeline(messages: List[dict],
                             remove_keywords: List[str] = None,
                             filter_keywords: List[str] = None,
                             question_keywords: List[str] = None,
                             template: Optional[dict] = None,
                             important_score: Optional[float] = 0.7,
                             batch_size: int = 50,
                             provider: Literal['google', 'groq'] = 'groq'):
    """
    Analyzes a list of messages using various LLM-based pipelines.

    This function takes a list of messages and performs a series of analyses, including:
    - Keyword filtering
    - Template message handling
    - Inquiry classification
    - User and purpose extraction
    - Question classification

    Args:
        messages (List[dict]): A list of messages to be analyzed.
        remove_keywords (List[str], optional): A list of keywords to remove from messages. Defaults to None.
        filter_keywords (List[str], optional): A list of keywords to filter messages by. Defaults to None.
        question_keywords (List[str], optional): A list of keywords to identify questions. Defaults to None.
        template (Optional[dict], optional): A dictionary of template messages. Defaults to None.
        important_score (Optional[float], optional): The minimum score threshold for classifying an inquiry. Defaults to 0.7.
        batch_size (int, optional): The number of messages to process in each batch. Defaults to 50.
        provider (Literal['google', 'groq'], optional): The LLM provider to use. Defaults to 'groq'.

    Returns:
        Tuple[List[dict], List[dict], List[dict]]: A tuple containing three lists:
            - `extracted_messages`: A list of messages with extracted user and purpose information.
            - `questions`: A list of messages classified as questions.
            - `error_messages`: A list of messages that encountered errors during processing.
    """
    # Initialize results
    extracted_messages = []
    error_messages = []

    # start processing
    if remove_keywords:
        messages = keyword_filter(remove_keywords, messages, get_keyword=False)

    if filter_keywords:
        messages = keyword_filter(filter_keywords, messages, get_keyword=True)

    template_messages = None
    if template is not None:
        template_messages, messages = handle_template_message(template, messages)
        extracted_messages += template_messages

    # filter to avoid exceed limit tokens
    messages = messages[-400:]
    error_messages = messages[:-400]

    # extract user and purpose
    classified_mess, error = classify_inquiry_pipeline(messages, important_score, batch_size, provider)
    error_messages += error

    extracted_mess, error = extract_user_purpose_pipeline(classified_mess, batch_size, provider)
    extracted_messages += extracted_mess
    error_messages += error

    # classifiy important questions
    if question_keywords is not None:
        messages = keyword_filter(question_keywords, messages, get_keyword=True)

    questions, error = classify_question_pipeline(messages, batch_size=batch_size, provider=provider)
    error_messages += error

    # deduplicate error messages
    error_messages = [json.loads(item) for item in {json.dumps(d, sort_keys=True) for d in error_messages}]
    
    return extracted_messages, questions, error_messages