{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(os.path.join(os.getcwd(), '../src'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from threading import Lock\n",
    "from typing import Dict\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "\n",
    "from prompt import inquiry_classifying_prompt, keyword_extracting_prompt\n",
    "from utils import *\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing Pipeline\n",
    "\n",
    "In this step, we will apply 2 methods to extract insightful data from customer's message:\n",
    "\n",
    "- **Meaningful inquiries**: Use LLM to detect any important, insightful customer's inquiries about products.\n",
    "- **Extracting keyword**: Use LLM to distil important keywords in messages\n",
    "\n",
    "We will combine these two methods into a complete pipeline to extract valuable information from customer messages. This pipeline will first classify messages as insightful inquiries, and then extract keywords from those classified messages. This approach allows us to focus on the most relevant information and gain deeper insights into customer needs and preferences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LOAD DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = load_json('../backup_data/total_message.json')\n",
    "customer_messages = [m for m in messages if m['from'] == 'customer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "START = 0\n",
    "N = 1500\n",
    "\n",
    "sample = customer_messages[START : START + N]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LOAD LLM**\n",
    "\n",
    "We will use ***Gemini-1.5-flash*** of Google, which is one of the state of the art LLMs (or even Multimodal model) in the present. Furthermore, this model is also provided a good API capacity for free tier.\n",
    "\n",
    "Because of requirement of precision and static output, we also need to modify `temperature`, `top_p`, and `top_k` to ensure model work accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_yaml('../config.yaml')\n",
    "LLM_CONFIG = config['llm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMCaller:\n",
    "    \"\"\"\n",
    "    A class to manage the rate of requests to an LLM.\n",
    "    \n",
    "    This class implements a simple rate limiting mechanism to prevent exceeding the maximum number of requests per minute allowed by the LLM API.\n",
    "    \n",
    "    Attributes:\n",
    "        max_request_per_minute (int): The maximum number of requests allowed per minute.\n",
    "        _request_counter (int): The number of requests made in the current minute.\n",
    "        _last_reset_time (float): The timestamp of the last time the request counter was reset.\n",
    "        _state_lock (Lock): A lock to protect the request counter and last reset time from concurrent access.\n",
    "    \"\"\"\n",
    "    _request_counter = 0\n",
    "    _last_reset_time = 0.0\n",
    "    _state_lock = Lock()\n",
    "\n",
    "    def __init__(self, max_request_per_minute: int):\n",
    "        self.max_request_per_minute = max_request_per_minute\n",
    "\n",
    "    def _reset_counter(self):\n",
    "        current_time = time.time()\n",
    "        if self._last_reset_time == 0.0 or current_time - self._last_reset_time >= 60:\n",
    "            self._request_counter = 0\n",
    "            self._last_reset_time = current_time\n",
    "\n",
    "\n",
    "    def _increment_counter(self, num_request):\n",
    "        with self._state_lock:\n",
    "            self._reset_counter()\n",
    "            if self._request_counter + num_request > self.max_request_per_minute:\n",
    "                time.sleep(max(0, self._last_reset_time + 60 - time.time()))\n",
    "                self._reset_counter()\n",
    "            self._request_counter += num_request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroqAICaller(LLMCaller):\n",
    "    def __init__(self, llm_config: dict, prompt: ChatPromptTemplate):\n",
    "        super().__init__(max_request_per_minute=30)\n",
    "\n",
    "        llm = ChatGroq(**llm_config)\n",
    "        self.chain = prompt | llm\n",
    "\n",
    "    def invoke(self, input: dict):\n",
    "        self._increment_counter(1)\n",
    "\n",
    "        return self.chain.invoke(input).content\n",
    "\n",
    "\n",
    "class GoogleAICaller(LLMCaller):\n",
    "    def __init__(self, llm_config: dict, prompt: PromptTemplate):\n",
    "        super().__init__(max_request_per_minute=15)\n",
    "\n",
    "        llm = GoogleGenerativeAI(**llm_config)\n",
    "        self.chain = prompt | llm\n",
    "\n",
    "\n",
    "    def invoke(self, input: dict):\n",
    "        self._increment_counter(1)\n",
    "\n",
    "        result = self.chain.invoke(input)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_llm_output(output: str):\n",
    "    \"\"\"\n",
    "    Parse the output of the LLM.\n",
    "    \n",
    "    The output of the LLM is expected to be in either '```python' or '```json' format.\n",
    "    This function will parse the output and return the result as a dictionary.\n",
    "    \n",
    "    Args:\n",
    "        output (str): The output of the LLM.\n",
    "    \n",
    "    Returns:\n",
    "        dict: The parsed output of the LLM.\n",
    "    \n",
    "    Raises:\n",
    "        Exception: If the output is not in the expected format.\n",
    "    \"\"\"\n",
    "    start = output.index('[')\n",
    "    end =  len(output) - output[::-1].index(']')\n",
    "\n",
    "    error_comma = end - 2 if output[end - 1] == ',' else end - 3\n",
    "    if output[error_comma] == ',':\n",
    "        output = output[:error_comma] + output[error_comma + 1:]\n",
    "\n",
    "    try:\n",
    "        res = json.loads(output[start:end])\n",
    "    except Exception:\n",
    "        try:\n",
    "            res = json.loads(output[start:end].lower())\n",
    "        except Exception:\n",
    "            raise Exception(f\"Could not parse output. Received: \\n{output}\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter by pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_filter(patterns: List[str], messages: List[dict], get_keyword: Optional[bool] = True) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Filter messages based on the presence or absence of specified keywords.\n",
    "\n",
    "    This function iterates through a list of messages and checks if each message contains any of the given keywords.\n",
    "    It returns a list of messages that either contain or do not contain the specified keywords, depending on the `get_keyword` flag.\n",
    "\n",
    "    Args:\n",
    "        patterns (List[str]): A list of keywords to filter by.\n",
    "        messages (List[dict]): A list of messages to filter.\n",
    "        get_keyword (Optional[bool], optional): If True, returns messages containing the keywords. \n",
    "    If False, returns messages not containing the keywords. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        List[dict]: A list of messages that meet the filtering criteria.\n",
    "    \"\"\"\n",
    "    synthetic_pattern = r'\\b(' + '|'.join(patterns) + r')\\b'\n",
    "    result = [m for m in messages \n",
    "              if bool(re.search(synthetic_pattern, m['message'].lower())) == get_keyword]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Inquiries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_inquiry_pipeline(\n",
    "    messages: List[dict],\n",
    "    min_score: float,\n",
    "    batch_size: int = 50,\n",
    "    provider: Literal[\"google\", \"groq\"] = \"groq\",\n",
    ") -> Tuple[List[dict], List[dict]]:\n",
    "    # classify by LLM\n",
    "    if provider == \"google\":\n",
    "        chain = GoogleAICaller(LLM_CONFIG[provider], inquiry_classifying_prompt)\n",
    "    else:\n",
    "        chain = GroqAICaller(LLM_CONFIG[provider], inquiry_classifying_prompt)\n",
    "\n",
    "    mask = []\n",
    "    with ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:\n",
    "        future = {\n",
    "            executor.submit(\n",
    "                lambda : chain.invoke({\"input\": str([m[\"message\"] for m in messages[i : min(i + batch_size, len(messages))]])})\n",
    "            ): i\n",
    "            for i in range(0, len(messages), batch_size)\n",
    "        }\n",
    "        for f in tqdm(as_completed(future), total=len(future), desc=\"Detecting insightful inquiry\"):\n",
    "            i = future[f]\n",
    "            end_idx = min(i + batch_size, len(messages))\n",
    "            try:\n",
    "                response = f.result()\n",
    "            except Exception:\n",
    "                print(f\"Error while generating response for batch {i} - {end_idx}\")\n",
    "                print(response)\n",
    "                mask += [\"error\" for _ in range(i, end_idx)]\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                parsed_response = _parse_llm_output(response)\n",
    "                mask += [list(r.items())[0][1] for r in parsed_response]\n",
    "            except Exception:\n",
    "                print(f\"Error while parsing LLM output for batch {i} - {end_idx}\")\n",
    "                print(response)\n",
    "                mask += [\"error\" for _ in range(i, end_idx)]\n",
    "\n",
    "    # get output to return\n",
    "    classified_messages = [m for m, l in zip(messages, mask) if l != 'error' and l >= min_score]\n",
    "    error_messages = [m for m, l in zip(messages, mask) if l == \"error\"]\n",
    "\n",
    "    return classified_messages, error_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_template_message(templates: Dict[str, Dict[str, str]], messages: List[dict]) -> Tuple[List[dict], List[dict]]:\n",
    "    \"\"\"\n",
    "    Handle template messages.\n",
    "\n",
    "    This function iterates through a list of messages and checks if each message is a key in the `templates` dictionary.\n",
    "    If a message is found in the `templates` dictionary, it updates the message with the corresponding template information\n",
    "    and appends it to the `template_message` list. Otherwise, it appends the message to the `other_message` list.\n",
    "\n",
    "    Args:\n",
    "        templates (Dict[str, Dict[str, str]]): A dictionary of template messages, where the key is the message string\n",
    "            and the value is a dictionary containing the user and purpose information.\n",
    "        messages (List[dict]): A list of messages to be processed.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[dict], List[dict]]: A tuple containing two lists:\n",
    "            - `template_message`: A list of messages that were found in the `templates` dictionary.\n",
    "            - `other_message`: A list of messages that were not found in the `templates` dictionary.\n",
    "    \"\"\"\n",
    "    template_message = []\n",
    "    other_message = []\n",
    "    for m in messages:\n",
    "        key = m['message'].lower()\n",
    "        if key in templates:\n",
    "            m.update(templates[key])\n",
    "            template_message.append(m)\n",
    "        else:\n",
    "            other_message.append(m)\n",
    "\n",
    "    return template_message, other_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keyword_pipeline(messages: List, \n",
    "                             batch_size: int = 50, \n",
    "                             provider: Literal['google', 'groq'] = 'groq') -> Tuple[List[dict], List[dict]]:\n",
    "    if provider == 'google':\n",
    "        chain = GoogleAICaller(LLM_CONFIG[provider], keyword_extracting_prompt)\n",
    "    else:\n",
    "        chain = GroqAICaller(LLM_CONFIG[provider], keyword_extracting_prompt)\n",
    "    \n",
    "    keywords = []\n",
    "    with ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:\n",
    "        future = {\n",
    "            executor.submit(\n",
    "                lambda : chain.invoke({\"input\": str([m[\"message\"] for m in messages[i : min(i + batch_size, len(messages))]])})\n",
    "            ): i\n",
    "            for i in range(0, len(messages), batch_size)\n",
    "        }\n",
    "        for f in tqdm(as_completed(future), total=len(future), desc='Extracting keywords'):\n",
    "            i = future[f]\n",
    "            end_idx = min(len(messages), i + batch_size)\n",
    "            try:\n",
    "                response = f.result()\n",
    "            \n",
    "            except Exception:\n",
    "                print(f'Error while generating response for batch {i} - {end_idx}')\n",
    "                keywords += ['error' for _ in range(i, end_idx)]\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                parsed_response = _parse_llm_output(response)\n",
    "                keywords += parsed_response\n",
    "\n",
    "            except Exception as exc:\n",
    "                print(f'Error while parsing LLM output for batch {i} - {end_idx}: {exc}')\n",
    "                keywords += ['error' for _ in range(i, end_idx)]\n",
    "\n",
    "    extracted_messages = []\n",
    "    error_messages = []\n",
    "    for mess, kw_item in zip(messages, keywords):\n",
    "        if kw_item == 'error':\n",
    "            error_messages.append(mess)\n",
    "        \n",
    "        extracted_messages.append(mess)\n",
    "\n",
    "    return extracted_messages, error_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entire Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_message_pipeline(messages: List[dict],\n",
    "                             filter_patterns: Optional[List[str]] = None, \n",
    "                             template: Optional[dict] = None,\n",
    "                             important_score: Optional[float] = 0.7,\n",
    "                             batch_size: int = 30,\n",
    "                             provider: Literal['google', 'groq'] = 'groq'):\n",
    "    # Initialize results\n",
    "    processed_messages = []\n",
    "    error_messages = []\n",
    "\n",
    "    # start processing\n",
    "    if filter_patterns is not None:\n",
    "        messages = keyword_filter_message(filter_patterns, messages)\n",
    "\n",
    "    template_message = None\n",
    "    if template is not None:\n",
    "        template_messages, messages = handle_template_message(template, messages)\n",
    "\n",
    "    messages, error = classify_inquiry_pipeline(messages, important_score, batch_size, provider)\n",
    "    error_messages += error\n",
    "\n",
    "    messages, error = extract_keyword_pipeline(messages, batch_size, provider)\n",
    "    error_messages += error\n",
    "    processed_messages += messages\n",
    "\n",
    "    return template_messages, processed_messages, error_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_messages = []\n",
    "error_messages = []\n",
    "\n",
    "# start processing\n",
    "if filter_patterns is not None:\n",
    "    messages = keyword_filter_message(filter_patterns, messages)\n",
    "\n",
    "template_message = None\n",
    "if template is not None:\n",
    "    template_messages, messages = handle_template_message(template, messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c09d950ecf248b58f56ba2fb2005aef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Detecting insightful inquiry:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while generating response for batch 510 - 540\n",
      "[{\"Đặt rồi\": 0.1}, {\"Giao chưa e\": 0.1}, {\"Sao không thấy shipper liên hệ\": 0.1}, {\"Fb này\": 0.0}, {\"Sao kỳ vậy\": 0.0}, {\"C nói 10h mà\": 0.1}, {\"Sáng giờ c nhắc mấy lần rồi\": 0.1}, {\"Sao không nói sớm đợi hỏi mấy lần mới nói\": 0.1}, {\"Đặt từ tối qua r\": 0.1}, {\"Trễ là mấy giờ\": 0.1}, {\"Nhanh đi e\": 0.1}]\n",
      "Error while generating response for batch 600 - 630\n",
      "[{\"Đặt rồi\": 0.1}, {\"Giao chưa e\": 0.1}, {\"Sao không thấy shipper liên hệ\": 0.1}, {\"Fb này\": 0.0}, {\"Sao kỳ vậy\": 0.0}, {\"C nói 10h mà\": 0.1}, {\"Sáng giờ c nhắc mấy lần rồi\": 0.1}, {\"Sao không nói sớm đợi hỏi mấy lần mới nói\": 0.1}, {\"Đặt từ tối qua r\": 0.1}, {\"Trễ là mấy giờ\": 0.1}, {\"Nhanh đi e\": 0.1}]\n",
      "Error while generating response for batch 570 - 600\n",
      "[{\"Đặt rồi\": 0.1}, {\"Giao chưa e\": 0.1}, {\"Sao không thấy shipper liên hệ\": 0.1}, {\"Fb này\": 0.0}, {\"Sao kỳ vậy\": 0.0}, {\"C nói 10h mà\": 0.1}, {\"Sáng giờ c nhắc mấy lần rồi\": 0.1}, {\"Sao không nói sớm đợi hỏi mấy lần mới nói\": 0.1}, {\"Đặt từ tối qua r\": 0.1}, {\"Trễ là mấy giờ\": 0.1}, {\"Nhanh đi e\": 0.1}]\n",
      "Error while generating response for batch 540 - 570\n",
      "[{\"Đặt rồi\": 0.1}, {\"Giao chưa e\": 0.1}, {\"Sao không thấy shipper liên hệ\": 0.1}, {\"Fb này\": 0.0}, {\"Sao kỳ vậy\": 0.0}, {\"C nói 10h mà\": 0.1}, {\"Sáng giờ c nhắc mấy lần rồi\": 0.1}, {\"Sao không nói sớm đợi hỏi mấy lần mới nói\": 0.1}, {\"Đặt từ tối qua r\": 0.1}, {\"Trễ là mấy giờ\": 0.1}, {\"Nhanh đi e\": 0.1}]\n",
      "Error while generating response for batch 480 - 510\n",
      "[{\"Đặt rồi\": 0.1}, {\"Giao chưa e\": 0.1}, {\"Sao không thấy shipper liên hệ\": 0.1}, {\"Fb này\": 0.0}, {\"Sao kỳ vậy\": 0.0}, {\"C nói 10h mà\": 0.1}, {\"Sáng giờ c nhắc mấy lần rồi\": 0.1}, {\"Sao không nói sớm đợi hỏi mấy lần mới nói\": 0.1}, {\"Đặt từ tối qua r\": 0.1}, {\"Trễ là mấy giờ\": 0.1}, {\"Nhanh đi e\": 0.1}]\n",
      "Error while generating response for batch 630 - 660\n",
      "[{\"Đặt rồi\": 0.1}, {\"Giao chưa e\": 0.1}, {\"Sao không thấy shipper liên hệ\": 0.1}, {\"Fb này\": 0.0}, {\"Sao kỳ vậy\": 0.0}, {\"C nói 10h mà\": 0.1}, {\"Sáng giờ c nhắc mấy lần rồi\": 0.1}, {\"Sao không nói sớm đợi hỏi mấy lần mới nói\": 0.1}, {\"Đặt từ tối qua r\": 0.1}, {\"Trễ là mấy giờ\": 0.1}, {\"Nhanh đi e\": 0.1}]\n",
      "Error while generating response for batch 660 - 690\n",
      "[{\"Đặt rồi\": 0.1}, {\"Giao chưa e\": 0.1}, {\"Sao không thấy shipper liên hệ\": 0.1}, {\"Fb này\": 0.0}, {\"Sao kỳ vậy\": 0.0}, {\"C nói 10h mà\": 0.1}, {\"Sáng giờ c nhắc mấy lần rồi\": 0.1}, {\"Sao không nói sớm đợi hỏi mấy lần mới nói\": 0.1}, {\"Đặt từ tối qua r\": 0.1}, {\"Trễ là mấy giờ\": 0.1}, {\"Nhanh đi e\": 0.1}]\n",
      "Error while generating response for batch 690 - 720\n",
      "[{\"Đặt rồi\": 0.1}, {\"Giao chưa e\": 0.1}, {\"Sao không thấy shipper liên hệ\": 0.1}, {\"Fb này\": 0.0}, {\"Sao kỳ vậy\": 0.0}, {\"C nói 10h mà\": 0.1}, {\"Sáng giờ c nhắc mấy lần rồi\": 0.1}, {\"Sao không nói sớm đợi hỏi mấy lần mới nói\": 0.1}, {\"Đặt từ tối qua r\": 0.1}, {\"Trễ là mấy giờ\": 0.1}, {\"Nhanh đi e\": 0.1}]\n",
      "Error while generating response for batch 720 - 750\n",
      "[{\"Đặt rồi\": 0.1}, {\"Giao chưa e\": 0.1}, {\"Sao không thấy shipper liên hệ\": 0.1}, {\"Fb này\": 0.0}, {\"Sao kỳ vậy\": 0.0}, {\"C nói 10h mà\": 0.1}, {\"Sáng giờ c nhắc mấy lần rồi\": 0.1}, {\"Sao không nói sớm đợi hỏi mấy lần mới nói\": 0.1}, {\"Đặt từ tối qua r\": 0.1}, {\"Trễ là mấy giờ\": 0.1}, {\"Nhanh đi e\": 0.1}]\n",
      "Error while generating response for batch 750 - 780\n",
      "[{\"Đặt rồi\": 0.1}, {\"Giao chưa e\": 0.1}, {\"Sao không thấy shipper liên hệ\": 0.1}, {\"Fb này\": 0.0}, {\"Sao kỳ vậy\": 0.0}, {\"C nói 10h mà\": 0.1}, {\"Sáng giờ c nhắc mấy lần rồi\": 0.1}, {\"Sao không nói sớm đợi hỏi mấy lần mới nói\": 0.1}, {\"Đặt từ tối qua r\": 0.1}, {\"Trễ là mấy giờ\": 0.1}, {\"Nhanh đi e\": 0.1}]\n",
      "Error while generating response for batch 840 - 870\n",
      "[{\"Đặt rồi\": 0.1}, {\"Giao chưa e\": 0.1}, {\"Sao không thấy shipper liên hệ\": 0.1}, {\"Fb này\": 0.0}, {\"Sao kỳ vậy\": 0.0}, {\"C nói 10h mà\": 0.1}, {\"Sáng giờ c nhắc mấy lần rồi\": 0.1}, {\"Sao không nói sớm đợi hỏi mấy lần mới nói\": 0.1}, {\"Đặt từ tối qua r\": 0.1}, {\"Trễ là mấy giờ\": 0.1}, {\"Nhanh đi e\": 0.1}]\n",
      "Error while generating response for batch 810 - 840\n",
      "[{\"Đặt rồi\": 0.1}, {\"Giao chưa e\": 0.1}, {\"Sao không thấy shipper liên hệ\": 0.1}, {\"Fb này\": 0.0}, {\"Sao kỳ vậy\": 0.0}, {\"C nói 10h mà\": 0.1}, {\"Sáng giờ c nhắc mấy lần rồi\": 0.1}, {\"Sao không nói sớm đợi hỏi mấy lần mới nói\": 0.1}, {\"Đặt từ tối qua r\": 0.1}, {\"Trễ là mấy giờ\": 0.1}, {\"Nhanh đi e\": 0.1}]\n",
      "Error while generating response for batch 780 - 810\n",
      "[{\"Đặt rồi\": 0.1}, {\"Giao chưa e\": 0.1}, {\"Sao không thấy shipper liên hệ\": 0.1}, {\"Fb này\": 0.0}, {\"Sao kỳ vậy\": 0.0}, {\"C nói 10h mà\": 0.1}, {\"Sáng giờ c nhắc mấy lần rồi\": 0.1}, {\"Sao không nói sớm đợi hỏi mấy lần mới nói\": 0.1}, {\"Đặt từ tối qua r\": 0.1}, {\"Trễ là mấy giờ\": 0.1}, {\"Nhanh đi e\": 0.1}]\n",
      "Error while generating response for batch 870 - 900\n",
      "[{\"Đặt rồi\": 0.1}, {\"Giao chưa e\": 0.1}, {\"Sao không thấy shipper liên hệ\": 0.1}, {\"Fb này\": 0.0}, {\"Sao kỳ vậy\": 0.0}, {\"C nói 10h mà\": 0.1}, {\"Sáng giờ c nhắc mấy lần rồi\": 0.1}, {\"Sao không nói sớm đợi hỏi mấy lần mới nói\": 0.1}, {\"Đặt từ tối qua r\": 0.1}, {\"Trễ là mấy giờ\": 0.1}, {\"Nhanh đi e\": 0.1}]\n",
      "Error while generating response for batch 900 - 930\n",
      "[{\"Đặt rồi\": 0.1}, {\"Giao chưa e\": 0.1}, {\"Sao không thấy shipper liên hệ\": 0.1}, {\"Fb này\": 0.0}, {\"Sao kỳ vậy\": 0.0}, {\"C nói 10h mà\": 0.1}, {\"Sáng giờ c nhắc mấy lần rồi\": 0.1}, {\"Sao không nói sớm đợi hỏi mấy lần mới nói\": 0.1}, {\"Đặt từ tối qua r\": 0.1}, {\"Trễ là mấy giờ\": 0.1}, {\"Nhanh đi e\": 0.1}]\n",
      "Error while generating response for batch 930 - 960\n",
      "[{\"Đặt rồi\": 0.1}, {\"Giao chưa e\": 0.1}, {\"Sao không thấy shipper liên hệ\": 0.1}, {\"Fb này\": 0.0}, {\"Sao kỳ vậy\": 0.0}, {\"C nói 10h mà\": 0.1}, {\"Sáng giờ c nhắc mấy lần rồi\": 0.1}, {\"Sao không nói sớm đợi hỏi mấy lần mới nói\": 0.1}, {\"Đặt từ tối qua r\": 0.1}, {\"Trễ là mấy giờ\": 0.1}, {\"Nhanh đi e\": 0.1}]\n",
      "Error while generating response for batch 960 - 990\n",
      "[{\"Đặt rồi\": 0.1}, {\"Giao chưa e\": 0.1}, {\"Sao không thấy shipper liên hệ\": 0.1}, {\"Fb này\": 0.0}, {\"Sao kỳ vậy\": 0.0}, {\"C nói 10h mà\": 0.1}, {\"Sáng giờ c nhắc mấy lần rồi\": 0.1}, {\"Sao không nói sớm đợi hỏi mấy lần mới nói\": 0.1}, {\"Đặt từ tối qua r\": 0.1}, {\"Trễ là mấy giờ\": 0.1}, {\"Nhanh đi e\": 0.1}]\n",
      "Error while generating response for batch 990 - 1020\n",
      "[{\"Đặt rồi\": 0.1}, {\"Giao chưa e\": 0.1}, {\"Sao không thấy shipper liên hệ\": 0.1}, {\"Fb này\": 0.0}, {\"Sao kỳ vậy\": 0.0}, {\"C nói 10h mà\": 0.1}, {\"Sáng giờ c nhắc mấy lần rồi\": 0.1}, {\"Sao không nói sớm đợi hỏi mấy lần mới nói\": 0.1}, {\"Đặt từ tối qua r\": 0.1}, {\"Trễ là mấy giờ\": 0.1}, {\"Nhanh đi e\": 0.1}]\n",
      "Error while generating response for batch 1050 - 1080\n",
      "[{\"Đặt rồi\": 0.1}, {\"Giao chưa e\": 0.1}, {\"Sao không thấy shipper liên hệ\": 0.1}, {\"Fb này\": 0.0}, {\"Sao kỳ vậy\": 0.0}, {\"C nói 10h mà\": 0.1}, {\"Sáng giờ c nhắc mấy lần rồi\": 0.1}, {\"Sao không nói sớm đợi hỏi mấy lần mới nói\": 0.1}, {\"Đặt từ tối qua r\": 0.1}, {\"Trễ là mấy giờ\": 0.1}, {\"Nhanh đi e\": 0.1}]\n",
      "Error while generating response for batch 1080 - 1110\n",
      "[{\"Đặt rồi\": 0.1}, {\"Giao chưa e\": 0.1}, {\"Sao không thấy shipper liên hệ\": 0.1}, {\"Fb này\": 0.0}, {\"Sao kỳ vậy\": 0.0}, {\"C nói 10h mà\": 0.1}, {\"Sáng giờ c nhắc mấy lần rồi\": 0.1}, {\"Sao không nói sớm đợi hỏi mấy lần mới nói\": 0.1}, {\"Đặt từ tối qua r\": 0.1}, {\"Trễ là mấy giờ\": 0.1}, {\"Nhanh đi e\": 0.1}]\n",
      "Error while generating response for batch 1020 - 1050\n",
      "[{\"Đặt rồi\": 0.1}, {\"Giao chưa e\": 0.1}, {\"Sao không thấy shipper liên hệ\": 0.1}, {\"Fb này\": 0.0}, {\"Sao kỳ vậy\": 0.0}, {\"C nói 10h mà\": 0.1}, {\"Sáng giờ c nhắc mấy lần rồi\": 0.1}, {\"Sao không nói sớm đợi hỏi mấy lần mới nói\": 0.1}, {\"Đặt từ tối qua r\": 0.1}, {\"Trễ là mấy giờ\": 0.1}, {\"Nhanh đi e\": 0.1}]\n",
      "Error while generating response for batch 1140 - 1151\n",
      "[{\"Đặt rồi\": 0.1}, {\"Giao chưa e\": 0.1}, {\"Sao không thấy shipper liên hệ\": 0.1}, {\"Fb này\": 0.0}, {\"Sao kỳ vậy\": 0.0}, {\"C nói 10h mà\": 0.1}, {\"Sáng giờ c nhắc mấy lần rồi\": 0.1}, {\"Sao không nói sớm đợi hỏi mấy lần mới nói\": 0.1}, {\"Đặt từ tối qua r\": 0.1}, {\"Trễ là mấy giờ\": 0.1}, {\"Nhanh đi e\": 0.1}]\n",
      "Error while generating response for batch 1110 - 1140\n",
      "[{\"Đặt rồi\": 0.1}, {\"Giao chưa e\": 0.1}, {\"Sao không thấy shipper liên hệ\": 0.1}, {\"Fb này\": 0.0}, {\"Sao kỳ vậy\": 0.0}, {\"C nói 10h mà\": 0.1}, {\"Sáng giờ c nhắc mấy lần rồi\": 0.1}, {\"Sao không nói sớm đợi hỏi mấy lần mới nói\": 0.1}, {\"Đặt từ tối qua r\": 0.1}, {\"Trễ là mấy giờ\": 0.1}, {\"Nhanh đi e\": 0.1}]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ac859dc7a534b3b9ed89a797d5baf52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting keywords:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while generating response for batch 0 - 30\n"
     ]
    }
   ],
   "source": [
    "template_messages, processed_messages, error_messages = analyse_message_pipeline(\n",
    "    sample,\n",
    "    config['filter-message-keywords'],\n",
    "    config['template-message'],\n",
    "    important_score=config['important-score'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'message': 'Kịp ko ạ',\n",
       "  'inserted_at': '2024-08-07T07:56:21.000000',\n",
       "  'from': 'customer'},\n",
       " {'message': '.',\n",
       "  'inserted_at': '2024-08-21T09:08:29.000000',\n",
       "  'from': 'customer'},\n",
       " {'message': 'ở cam ranh ý b',\n",
       "  'inserted_at': '2024-08-21T09:21:07.000000',\n",
       "  'from': 'customer'},\n",
       " {'message': 'Mình mua dùng',\n",
       "  'inserted_at': '2024-08-23T02:06:04.000000',\n",
       "  'from': 'customer'},\n",
       " {'message': 'Cho mình 3 phần 196k nhé',\n",
       "  'inserted_at': '2024-08-23T02:06:39.000000',\n",
       "  'from': 'customer'},\n",
       " {'message': '167/8a đường 30/4 \\n0908866880',\n",
       "  'inserted_at': '2024-08-23T02:07:14.000000',\n",
       "  'from': 'customer'},\n",
       " {'message': 'Alo',\n",
       "  'inserted_at': '2024-08-12T13:30:46.000000',\n",
       "  'from': 'customer'},\n",
       " {'message': 'Này có sẵn ko ạh',\n",
       "  'inserted_at': '2024-08-13T09:27:36.000000',\n",
       "  'from': 'customer'},\n",
       " {'message': 'Lấy mình 1 súp tiểu bảo',\n",
       "  'inserted_at': '2024-08-06T04:15:55.000000',\n",
       "  'from': 'customer'},\n",
       " {'message': 'Mình ở gần Long Hải hơn b ạ',\n",
       "  'inserted_at': '2024-08-06T04:17:30.000000',\n",
       "  'from': 'customer'},\n",
       " {'message': 'đúng rồi shop',\n",
       "  'inserted_at': '2024-08-27T11:52:17.000000',\n",
       "  'from': 'customer'},\n",
       " {'message': 'Phần càng long ạ',\n",
       "  'inserted_at': '2024-08-26T12:19:56.000000',\n",
       "  'from': 'customer'},\n",
       " {'message': 'Bao nhiu tiền 1 phần ạ',\n",
       "  'inserted_at': '2024-08-31T04:19:58.000000',\n",
       "  'from': 'customer'},\n",
       " {'message': 'Đợi mình xíu mha',\n",
       "  'inserted_at': '2024-08-23T02:51:03.000000',\n",
       "  'from': 'customer'},\n",
       " {'message': 'Súp Từ Hy và Càn Long',\n",
       "  'inserted_at': '2024-08-23T02:57:14.000000',\n",
       "  'from': 'customer'},\n",
       " {'message': 'Ib',\n",
       "  'inserted_at': '2024-08-17T08:24:02.000000',\n",
       "  'from': 'customer'},\n",
       " {'message': 'Thêm nhiu shop',\n",
       "  'inserted_at': '2024-08-06T15:02:31.000000',\n",
       "  'from': 'customer'},\n",
       " {'message': 'bào ngư vi cá có để ở ngoài cỡ 5 tiếng đc ko ạ',\n",
       "  'inserted_at': '2024-08-14T05:33:13.000000',\n",
       "  'from': 'customer'},\n",
       " {'message': 'bạch yến càng long và yến chưng tươi',\n",
       "  'inserted_at': '2024-08-08T07:19:53.000000',\n",
       "  'from': 'customer'},\n",
       " {'message': 'món bạch yến càn long này người già lớn tuổi ăn hợp không em',\n",
       "  'inserted_at': '2024-08-08T07:37:28.000000',\n",
       "  'from': 'customer'},\n",
       " {'message': 'bà 90 tuổi rồi có bị dai không',\n",
       "  'inserted_at': '2024-08-08T07:37:36.000000',\n",
       "  'from': 'customer'},\n",
       " {'message': 'Huynh Giau',\n",
       "  'inserted_at': '2024-08-10T14:01:50.000000',\n",
       "  'from': 'customer'},\n",
       " {'message': 'Đây ạ',\n",
       "  'inserted_at': '2024-08-27T11:40:32.000000',\n",
       "  'from': 'customer'},\n",
       " {'message': 'Dạ đúng rồi',\n",
       "  'inserted_at': '2024-08-27T11:47:08.000000',\n",
       "  'from': 'customer'},\n",
       " {'message': 'Chắc em ko hợp ăn món này',\n",
       "  'inserted_at': '2024-08-28T10:51:02.000000',\n",
       "  'from': 'customer'},\n",
       " {'message': 'phụ nữ sau sinh có dùng được k ạ',\n",
       "  'inserted_at': '2024-09-02T04:04:42.000000',\n",
       "  'from': 'customer'},\n",
       " {'message': 'Ngã 3 phùng quán với võ duy ninh',\n",
       "  'inserted_at': '2024-09-02T06:34:46.000000',\n",
       "  'from': 'customer'},\n",
       " {'message': 'Sđt e 0376734735',\n",
       "  'inserted_at': '2024-09-02T06:36:28.000000',\n",
       "  'from': 'customer'},\n",
       " {'message': 'E tặng ng nhà thôi',\n",
       "  'inserted_at': '2024-09-02T06:43:16.000000',\n",
       "  'from': 'customer'},\n",
       " {'message': 'Thế e ck nha',\n",
       "  'inserted_at': '2024-09-02T06:45:33.000000',\n",
       "  'from': 'customer'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_messages"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
