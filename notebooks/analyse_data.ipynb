{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(os.path.join(os.getcwd(), '../src'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from threading import Lock\n",
    "from typing import Dict\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "\n",
    "from prompt import classifying_inquiry_prompt, reclassifying_inquiry_prompt, extracting_user_purpose_prompt\n",
    "from utils import *\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing Pipeline\n",
    "\n",
    "In this step, we will apply 2 methods to extract insightful data from customer's message:\n",
    "\n",
    "- **Meaningful inquiries**: Use LLM to detect any important, insightful customer's inquiries about products.\n",
    "- **Extracting keyword**: Use LLM to distil important keywords in messages\n",
    "\n",
    "We will combine these two methods into a complete pipeline to extract valuable information from customer messages. This pipeline will first classify messages as insightful inquiries, and then extract keywords from those classified messages. This approach allows us to focus on the most relevant information and gain deeper insights into customer needs and preferences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = load_json('../backup_data/total_message.json')\n",
    "customer_messages = [m for m in messages if m['from'] == 'customer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "START = 0\n",
    "N = 1000\n",
    "\n",
    "sample = customer_messages[START : START + N]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load LLM\n",
    "\n",
    "We will use ***Gemini-1.5-flash*** of Google, which is one of the state of the art LLMs (or even Multimodal model) in the present. Furthermore, this model is also provided a good API capacity for free tier.\n",
    "\n",
    "Because of requirement of precision and static output, we also need to modify `temperature`, `top_p`, and `top_k` to ensure model work accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_yaml('../config.yaml')\n",
    "LLM_CONFIG = config['llm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMCaller:\n",
    "    \"\"\"\n",
    "    A class to manage the rate of requests to an LLM.\n",
    "    \n",
    "    This class implements a simple rate limiting mechanism to prevent exceeding the maximum number of requests per minute allowed by the LLM API.\n",
    "    \n",
    "    Attributes:\n",
    "        max_request_per_minute (int): The maximum number of requests allowed per minute.\n",
    "        _request_counter (int): The number of requests made in the current minute.\n",
    "        _last_reset_time (float): The timestamp of the last time the request counter was reset.\n",
    "        _state_lock (Lock): A lock to protect the request counter and last reset time from concurrent access.\n",
    "    \"\"\"\n",
    "    _request_counter = 0\n",
    "    _last_reset_time = 0.0\n",
    "    _state_lock = Lock()\n",
    "\n",
    "    def __init__(self, max_request_per_minute: int):\n",
    "        self.max_request_per_minute = max_request_per_minute\n",
    "\n",
    "    def _reset_counter(self) -> None:\n",
    "        current_time = time.time()\n",
    "        if self._last_reset_time == 0.0 or current_time - self._last_reset_time >= 60:\n",
    "            self._request_counter = 0\n",
    "            self._last_reset_time = current_time\n",
    "\n",
    "    def _wait_to_next_minute(self) -> None:\n",
    "        \"\"\"\n",
    "        Wait until the start of the next minute.\n",
    "        \"\"\"\n",
    "        wait_time = max(0, self._last_reset_time + 60 - time.time())\n",
    "        time.sleep(wait_time)\n",
    "        self._reset_counter()\n",
    "\n",
    "    def _increment_counter(self, num_request: int) -> None:\n",
    "        with self._state_lock:\n",
    "            self._reset_counter()\n",
    "            if self._request_counter + num_request > self.max_request_per_minute:\n",
    "                self._wait_to_next_minute()\n",
    "            self._request_counter += num_request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroqAICaller(LLMCaller):\n",
    "    def __init__(self, llm_config: dict, prompt: ChatPromptTemplate):\n",
    "        super().__init__(max_request_per_minute=30)\n",
    "        \n",
    "        config = {'max_retries': 0}\n",
    "        config.update(llm_config)\n",
    "\n",
    "        llm = ChatGroq(**config)\n",
    "        self.chain = prompt | llm\n",
    "\n",
    "    def _extract_error_code(self, exception: Exception) -> Optional[int]:\n",
    "        try:\n",
    "            error_code = exception.status_code\n",
    "        except Exception:\n",
    "            error_code = None\n",
    "        \n",
    "        return error_code\n",
    "\n",
    "    def invoke(self, input: dict) -> str:\n",
    "        self._increment_counter(1)\n",
    "        try:\n",
    "            result = self.chain.invoke(input).content\n",
    "        except Exception as exc:\n",
    "            if self._extract_error_code(exc) == 429:\n",
    "                print('Reaching maximum resources, wait to next minutes!')\n",
    "                self._wait_to_next_minute()\n",
    "            \n",
    "            result = self.chain.invoke(input).content\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "class GoogleAICaller(LLMCaller):\n",
    "    def __init__(self, llm_config: dict, prompt: PromptTemplate):\n",
    "        super().__init__(max_request_per_minute=15)\n",
    "        \n",
    "        config = {'max_retries': 0}\n",
    "        config.update(llm_config)\n",
    "        \n",
    "        llm = GoogleGenerativeAI(**config)\n",
    "        self.chain = prompt | llm\n",
    "\n",
    "    \n",
    "    def _extract_error_code(self, exception: Exception) -> Optional[int]:\n",
    "        try:\n",
    "            error_code = exception.code.value\n",
    "        except Exception:\n",
    "            error_code = None\n",
    "\n",
    "        return error_code\n",
    "        \n",
    "\n",
    "    def invoke(self, input: dict) -> str:\n",
    "        self._increment_counter(1)\n",
    "        try:\n",
    "            result = self.chain.invoke(input)\n",
    "        except Exception as exc:\n",
    "            if self._extract_error_code(exc) == 429:\n",
    "                print('Reaching maximum resources, wait to next minutes!')\n",
    "                self._wait_to_next_minute()\n",
    "                result = self.chain.invoke(input)\n",
    "\n",
    "            raise exc\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_llm_output(output: str):\n",
    "    \"\"\"\n",
    "    Parse the output of the LLM.\n",
    "    \n",
    "    The output of the LLM is expected to be in either '```python' or '```json' format.\n",
    "    This function will parse the output and return the result as a dictionary.\n",
    "    \n",
    "    Args:\n",
    "        output (str): The output of the LLM.\n",
    "    \n",
    "    Returns:\n",
    "        dict: The parsed output of the LLM.\n",
    "    \n",
    "    Raises:\n",
    "        Exception: If the output is not in the expected format.\n",
    "    \"\"\"\n",
    "    start = output.index('[')\n",
    "    end =  len(output) - output[::-1].index(']')\n",
    "\n",
    "    error_comma = end - 2 if output[end - 1] == ',' else end - 3\n",
    "    if output[error_comma] == ',':\n",
    "        output = output[:error_comma] + output[error_comma + 1:]\n",
    "\n",
    "    try:\n",
    "        res = json.loads(output[start:end])\n",
    "    except Exception:\n",
    "        try:\n",
    "            res = json.loads(output[start:end].lower())\n",
    "        except Exception:\n",
    "            raise Exception(f\"Could not parse output. Received: \\n{output}\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter by pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_filter(patterns: List[str], messages: List[dict], get_keyword: Optional[bool] = True) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Filter messages based on the presence or absence of specified keywords.\n",
    "\n",
    "    This function iterates through a list of messages and checks if each message contains any of the given keywords.\n",
    "    It returns a list of messages that either contain or do not contain the specified keywords, depending on the `get_keyword` flag.\n",
    "\n",
    "    Args:\n",
    "        patterns (List[str]): A list of keywords to filter by.\n",
    "        messages (List[dict]): A list of messages to filter.\n",
    "        get_keyword (Optional[bool], optional): If True, returns messages containing the keywords. \n",
    "            If False, returns messages not containing the keywords. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        List[dict]: A list of messages that meet the filtering criteria.\n",
    "    \"\"\"\n",
    "    synthetic_pattern = r'\\b(' + '|'.join(patterns) + r')\\b'\n",
    "    result = [m for m in messages \n",
    "              if bool(re.search(synthetic_pattern, m['message'].lower())) == get_keyword]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Inquiries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_inquiry_pipeline(\n",
    "    messages: List[dict],\n",
    "    min_score: float,\n",
    "    batch_size: int = 50,\n",
    "    provider: Literal[\"google\", \"groq\"] = \"groq\",\n",
    ") -> Tuple[List[dict], List[dict]]:\n",
    "    # classify by LLM\n",
    "    if provider == \"google\":\n",
    "        chain = GoogleAICaller(LLM_CONFIG[provider], classifying_inquiry_prompt)\n",
    "    else:\n",
    "        chain = GroqAICaller(LLM_CONFIG[provider], classifying_inquiry_prompt)\n",
    "\n",
    "    mask = [None for _ in range(len(messages))]\n",
    "    with ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:\n",
    "        future = {\n",
    "            executor.submit(\n",
    "                lambda : chain.invoke({\"input\": str([m[\"message\"] for m in messages[i : min(i + batch_size, len(messages))]])})\n",
    "            ): i\n",
    "            for i in range(0, len(messages), batch_size)\n",
    "        }\n",
    "        for f in tqdm(as_completed(future), total=len(future), desc=\"Detecting insightful inquiry\"):\n",
    "            i = future[f]\n",
    "            end_idx = min(i + batch_size, len(messages))\n",
    "            try:\n",
    "                response = f.result()\n",
    "            except Exception as exc:\n",
    "                print(f\"Error while generating response for batch {i} - {end_idx - 1}\")\n",
    "\n",
    "                # update mask\n",
    "                for i in range(i, end_idx):\n",
    "                    mask[i] = 'error'\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                parsed_response = _parse_llm_output(response)\n",
    "                # update mask\n",
    "                for i, idx in enumerate(range(i, end_idx)):\n",
    "                    mask[idx] = parsed_response[i][messages[idx]['message']]\n",
    "\n",
    "            except Exception:\n",
    "                print(f\"Error while parsing LLM output for batch {i} - {end_idx - 1}\")\n",
    "                \n",
    "                # update mask\n",
    "                for i in range(i, end_idx):\n",
    "                    mask[i] = 'error'\n",
    "                continue\n",
    "\n",
    "    # get output to return\n",
    "    classified_messages = [\n",
    "        m for m, l in zip(messages, mask) \n",
    "        if l != 'error' and l >= min_score\n",
    "    ]\n",
    "    error_messages = [m for m, l in zip(messages, mask) if l == \"error\"]\n",
    "\n",
    "    return classified_messages, error_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reclassify_inquiry_pipeline(\n",
    "    messages: List[dict],\n",
    "    min_score: float,\n",
    "    batch_size: int = 50,\n",
    "    provider: Literal[\"google\", \"groq\"] = \"groq\",\n",
    ") -> Tuple[List[dict], List[dict]]:\n",
    "    # classify by LLM\n",
    "    if provider == \"google\":\n",
    "        chain = GoogleAICaller(LLM_CONFIG[provider], reclassifying_inquiry_prompt)\n",
    "    else:\n",
    "        chain = GroqAICaller(LLM_CONFIG[provider], reclassifying_inquiry_prompt)\n",
    "\n",
    "    mask = [None for _ in range(len(messages))]\n",
    "    with ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:\n",
    "        future = {\n",
    "            executor.submit(\n",
    "                lambda : chain.invoke({\"input\": str([m[\"message\"] for m in messages[i : min(i + batch_size, len(messages))]])})\n",
    "            ): i\n",
    "            for i in range(0, len(messages), batch_size)\n",
    "        }\n",
    "        for f in tqdm(as_completed(future), total=len(future), desc=\"Detecting insightful inquiry\"):\n",
    "            i = future[f]\n",
    "            end_idx = min(i + batch_size, len(messages))\n",
    "            try:\n",
    "                response = f.result()\n",
    "            except Exception as exc:\n",
    "                print(f\"Error while generating response for batch {i} - {end_idx - 1}\")\n",
    "\n",
    "                # update mask\n",
    "                for i in range(i, end_idx):\n",
    "                    mask[i] = 'error'\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                parsed_response = _parse_llm_output(response)\n",
    "                # update mask\n",
    "                for i, idx in enumerate(range(i, end_idx)):\n",
    "                    mask[idx] = parsed_response[i][messages[idx]['message']]\n",
    "\n",
    "            except Exception:\n",
    "                print(f\"Error while parsing LLM output for batch {i} - {end_idx - 1}\")\n",
    "                \n",
    "                # update mask\n",
    "                for i in range(i, end_idx):\n",
    "                    mask[i] = 'error'\n",
    "                continue\n",
    "\n",
    "    # get output to return\n",
    "    classified_messages = [\n",
    "        m for m, l in zip(messages, mask) \n",
    "        if l != 'error' and l >= min_score\n",
    "    ]\n",
    "    error_messages = [m for m, l in zip(messages, mask) if l == \"error\"]\n",
    "\n",
    "    return classified_messages, error_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_template_message(templates: Dict[str, Dict[str, str]], messages: List[dict]) -> Tuple[List[dict], List[dict]]:\n",
    "    \"\"\"\n",
    "    Handle template messages.\n",
    "\n",
    "    This function iterates through a list of messages and checks if each message is a key in the `templates` dictionary.\n",
    "    If a message is found in the `templates` dictionary, it updates the message with the corresponding template information\n",
    "    and appends it to the `template_message` list. Otherwise, it appends the message to the `other_message` list.\n",
    "\n",
    "    Args:\n",
    "        templates (Dict[str, Dict[str, str]]): A dictionary of template messages, where the key is the message string\n",
    "            and the value is a dictionary containing the user and purpose information.\n",
    "        messages (List[dict]): A list of messages to be processed.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[dict], List[dict]]: A tuple containing two lists:\n",
    "            - `template_message`: A list of messages that were found in the `templates` dictionary.\n",
    "            - `other_message`: A list of messages that were not found in the `templates` dictionary.\n",
    "    \"\"\"\n",
    "    template_message = []\n",
    "    other_message = []\n",
    "    for m in messages:\n",
    "        key = m['message'].lower()\n",
    "        if key in templates:\n",
    "            m.update(templates[key])\n",
    "            template_message.append(m)\n",
    "        else:\n",
    "            other_message.append(m)\n",
    "\n",
    "    return template_message, other_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_user_purpose_pipeline(messages: List, \n",
    "                             batch_size: int = 50, \n",
    "                             provider: Literal['google', 'groq'] = 'groq') -> Tuple[List[dict], List[dict]]:\n",
    "    if provider == 'google':\n",
    "        chain = GoogleAICaller(LLM_CONFIG[provider], extracting_user_purpose_prompt)\n",
    "    else:\n",
    "        chain = GroqAICaller(LLM_CONFIG[provider], extracting_user_purpose_prompt)\n",
    "    \n",
    "    user_and_purpose = [None for _ in range(len(messages))]\n",
    "    with ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:\n",
    "        future = {\n",
    "            executor.submit(\n",
    "                lambda : chain.invoke({\"input\": str([m[\"message\"] for m in messages[i : min(i + batch_size, len(messages))]])})\n",
    "            ): i\n",
    "            for i in range(0, len(messages), batch_size)\n",
    "        }\n",
    "        for f in tqdm(as_completed(future), total=len(future), desc='Extracting keywords'):\n",
    "            i = future[f]\n",
    "            end_idx = min(len(messages), i + batch_size)\n",
    "            try:\n",
    "                response = f.result()\n",
    "            \n",
    "            except Exception:\n",
    "                print(f'Error while generating response for batch {i} - {end_idx - 1}')\n",
    "                # update `user_and_purpose`\n",
    "                for idx in range(i, end_idx):\n",
    "                    user_and_purpose[idx] = 'error'\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                parsed_response = _parse_llm_output(response)\n",
    "                # update `user_and_purpose`\n",
    "                for i, idx in enumerate(range(i, end_idx)):\n",
    "                    user_and_purpose[idx] = parsed_response[i][messages[idx]['message']].copy()\n",
    "\n",
    "            except Exception as exc:\n",
    "                print(f'Error while parsing LLM output for batch {i} - {end_idx - 1}')\n",
    "                # update `user_and_purpose`\n",
    "                for idx in range(i, end_idx):\n",
    "                    user_and_purpose[idx] = 'error'\n",
    "\n",
    "    extracted_messages = []\n",
    "    error_messages = []\n",
    "    for mess, u_and_p in zip(messages, user_and_purpose):\n",
    "        extracted_messages.append(mess.copy())\n",
    "        if u_and_p != 'error':\n",
    "            extracted_messages[-1].update(u_and_p)\n",
    "        \n",
    "    return extracted_messages, error_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entire Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_message_pipeline(messages: List[dict],\n",
    "                             remove_keywords: List[str] = None,\n",
    "                             filter_keywords: List[str] = None,\n",
    "                             template: Optional[dict] = None,\n",
    "                             important_score: Optional[float] = 0.7,\n",
    "                             batch_size: int = 50,\n",
    "                             provider: Literal['google', 'groq'] = 'groq'):\n",
    "    # Initialize results\n",
    "    processed_messages = []\n",
    "    error_messages = []\n",
    "\n",
    "    # start processing\n",
    "    if remove_keywords:\n",
    "        messages = keyword_filter(remove_keywords, messages, get_keyword=False)\n",
    "\n",
    "    if filter_keywords:\n",
    "        messages = keyword_filter(filter_keywords, messages, get_keyword=True)\n",
    "\n",
    "    template_messages = None\n",
    "    if template is not None:\n",
    "        template_messages, messages = handle_template_message(template, messages)\n",
    "\n",
    "    messages, error = classify_inquiry_pipeline(messages, important_score, batch_size, provider)\n",
    "    error_messages += error\n",
    "    messages, error = reclassify_inquiry_pipeline(messages, important_score, batch_size)\n",
    "    error_messages += error\n",
    "\n",
    "    messages, error = extract_user_purpose_pipeline(messages, batch_size, provider)\n",
    "    error_messages += error\n",
    "    processed_messages += messages\n",
    "\n",
    "    return template_messages, processed_messages, error_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1726857239.865356 1157140 fork_posix.cc:77] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b180dc43a14b4e03b50e7bb465931c62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Detecting insightful inquiry:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1726857240.515410 1157249 fork_posix.cc:77] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "I0000 00:00:1726857240.524004 1157248 fork_posix.cc:77] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a10b52ebe2747eab6c744d5dfb180ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Detecting insightful inquiry:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef3f304fb03d4940b63ab9f47ef191c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting keywords:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "important_keywords = config['product-keywords'] + config['important-message-keywords']\n",
    "\n",
    "template_messages, processed_messages, error_messages = analyse_message_pipeline(\n",
    "    sample,\n",
    "    remove_keywords=config['unimportant-message-keywords'],\n",
    "    filter_keywords=important_keywords,\n",
    "    template=config['template-message'],\n",
    "    important_score=config['important-score'],\n",
    "    provider='google'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
