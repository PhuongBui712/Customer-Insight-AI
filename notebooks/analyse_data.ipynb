{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(os.path.join(os.getcwd(), '../src'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from threading import Lock\n",
    "from typing import Dict\n",
    "from functools import lru_cache\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import List, Dict, Optional, Union\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "\n",
    "from prompt import (\n",
    "    classifying_inquiry_prompt,\n",
    "    reclassifying_inquiry_prompt,\n",
    "    classifying_important_question_prompt,\n",
    "    extracting_user_purpose_prompt\n",
    ")\n",
    "from utils import *\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ThreadPoolExecutor in module concurrent.futures.thread:\n",
      "\n",
      "class ThreadPoolExecutor(concurrent.futures._base.Executor)\n",
      " |  ThreadPoolExecutor(max_workers=None, thread_name_prefix='', initializer=None, initargs=())\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      ThreadPoolExecutor\n",
      " |      concurrent.futures._base.Executor\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(self, max_workers=None, thread_name_prefix='', initializer=None, initargs=())\n",
      " |      Initializes a new ThreadPoolExecutor instance.\n",
      " |\n",
      " |      Args:\n",
      " |          max_workers: The maximum number of threads that can be used to\n",
      " |              execute the given calls.\n",
      " |          thread_name_prefix: An optional name prefix to give our threads.\n",
      " |          initializer: A callable used to initialize worker threads.\n",
      " |          initargs: A tuple of arguments to pass to the initializer.\n",
      " |\n",
      " |  shutdown(self, wait=True, *, cancel_futures=False)\n",
      " |      Clean-up the resources associated with the Executor.\n",
      " |\n",
      " |      It is safe to call this method several times. Otherwise, no other\n",
      " |      methods can be called after this one.\n",
      " |\n",
      " |      Args:\n",
      " |          wait: If True then shutdown will not return until all running\n",
      " |              futures have finished executing and the resources used by the\n",
      " |              executor have been reclaimed.\n",
      " |          cancel_futures: If True then shutdown will cancel all pending\n",
      " |              futures. Futures that are completed or running will not be\n",
      " |              cancelled.\n",
      " |\n",
      " |  submit(self, fn, /, *args, **kwargs)\n",
      " |      Submits a callable to be executed with the given arguments.\n",
      " |\n",
      " |      Schedules the callable to be executed as fn(*args, **kwargs) and returns\n",
      " |      a Future instance representing the execution of the callable.\n",
      " |\n",
      " |      Returns:\n",
      " |          A Future representing the given call.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from concurrent.futures._base.Executor:\n",
      " |\n",
      " |  __enter__(self)\n",
      " |\n",
      " |  __exit__(self, exc_type, exc_val, exc_tb)\n",
      " |\n",
      " |  map(self, fn, *iterables, timeout=None, chunksize=1)\n",
      " |      Returns an iterator equivalent to map(fn, iter).\n",
      " |\n",
      " |      Args:\n",
      " |          fn: A callable that will take as many arguments as there are\n",
      " |              passed iterables.\n",
      " |          timeout: The maximum number of seconds to wait. If None, then there\n",
      " |              is no limit on the wait time.\n",
      " |          chunksize: The size of the chunks the iterable will be broken into\n",
      " |              before being passed to a child process. This argument is only\n",
      " |              used by ProcessPoolExecutor; it is ignored by\n",
      " |              ThreadPoolExecutor.\n",
      " |\n",
      " |      Returns:\n",
      " |          An iterator equivalent to: map(func, *iterables) but the calls may\n",
      " |          be evaluated out-of-order.\n",
      " |\n",
      " |      Raises:\n",
      " |          TimeoutError: If the entire result iterator could not be generated\n",
      " |              before the given timeout.\n",
      " |          Exception: If fn(*args) raises for any values.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from concurrent.futures._base.Executor:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ThreadPoolExecutor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing Pipeline\n",
    "\n",
    "In this step, we will apply 2 methods to extract insightful data from customer's message:\n",
    "\n",
    "- **Meaningful inquiries**: Use LLM to detect any important, insightful customer's inquiries about products.\n",
    "- **Extracting keyword**: Use LLM to distil important keywords in messages\n",
    "\n",
    "We will combine these two methods into a complete pipeline to extract valuable information from customer messages. This pipeline will first classify messages as insightful inquiries, and then extract keywords from those classified messages. This approach allows us to focus on the most relevant information and gain deeper insights into customer needs and preferences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = load_json('../backup_data/total_message.json')\n",
    "customer_messages = [m for m in messages if m['from'] == 'customer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "START = 0\n",
    "N = 3000\n",
    "\n",
    "sample = customer_messages[START : START + N]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load LLM\n",
    "\n",
    "We will use ***Gemini-1.5-flash*** of Google, which is one of the state of the art LLMs (or even Multimodal model) in the present. Furthermore, this model is also provided a good API capacity for free tier.\n",
    "\n",
    "Because of requirement of precision and static output, we also need to modify `temperature`, `top_p`, and `top_k` to ensure model work accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_yaml('../config.yaml')\n",
    "LLM_CONFIG = config['llm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMCaller:\n",
    "    \"\"\"\n",
    "    A class to manage the rate of requests to an LLM.\n",
    "    \n",
    "    This class implements a simple rate limiting mechanism to prevent exceeding the maximum number of requests per minute allowed by the LLM API.\n",
    "    \n",
    "    Attributes:\n",
    "        max_request_per_minute (int): The maximum number of requests allowed per minute.\n",
    "        _request_counter (int): The number of requests made in the current minute.\n",
    "        _last_reset_time (float): The timestamp of the last time the request counter was reset.\n",
    "        _state_lock (Lock): A lock to protect the request counter and last reset time from concurrent access.\n",
    "    \"\"\"\n",
    "    _request_counter = 0\n",
    "    _last_reset_time = 0.0\n",
    "    _state_lock = Lock()\n",
    "\n",
    "    def __init__(self, max_request_per_minute: int):\n",
    "        self.max_request_per_minute = max_request_per_minute\n",
    "\n",
    "    def _reset_counter(self) -> None:\n",
    "        current_time = time.time()\n",
    "        if self._last_reset_time == 0.0 or current_time - self._last_reset_time >= 60:\n",
    "            self._request_counter = 0\n",
    "            self._last_reset_time = current_time\n",
    "\n",
    "    def _wait_to_next_minute(self) -> None:\n",
    "        \"\"\"\n",
    "        Wait until the start of the next minute.\n",
    "        \"\"\"\n",
    "        wait_time = max(0, self._last_reset_time + 60 - time.time())\n",
    "        time.sleep(wait_time)\n",
    "        self._reset_counter()\n",
    "\n",
    "    def _increment_counter(self, num_request: int) -> None:\n",
    "        with self._state_lock:\n",
    "            self._reset_counter()\n",
    "            if self._request_counter + num_request > self.max_request_per_minute:\n",
    "                self._wait_to_next_minute()\n",
    "            self._request_counter += num_request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroqAICaller(LLMCaller):\n",
    "    def __init__(self, llm_config: dict, prompt: ChatPromptTemplate):\n",
    "        super().__init__(max_request_per_minute=30)\n",
    "        \n",
    "        config = {'max_retries': 0}\n",
    "        config.update(llm_config)\n",
    "\n",
    "        llm = ChatGroq(**config)\n",
    "        self.chain = prompt | llm\n",
    "\n",
    "    def _extract_error_code(self, exception: Exception) -> Optional[int]:\n",
    "        try:\n",
    "            error_code = exception.status_code\n",
    "        except Exception:\n",
    "            error_code = None\n",
    "        \n",
    "        return error_code\n",
    "\n",
    "    def invoke(self, input: dict) -> str:\n",
    "        self._increment_counter(1)\n",
    "        try:\n",
    "            result = self.chain.invoke(input).content\n",
    "        except Exception as exc:\n",
    "            if self._extract_error_code(exc) == 429:\n",
    "                print('Reaching maximum resources, wait to next minutes!')\n",
    "                self._wait_to_next_minute()\n",
    "            \n",
    "            result = self.chain.invoke(input).content\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "class GoogleAICaller(LLMCaller):\n",
    "    def __init__(self, llm_config: dict, prompt: PromptTemplate):\n",
    "        super().__init__(max_request_per_minute=15)\n",
    "        \n",
    "        config = {'max_retries': 0}\n",
    "        config.update(llm_config)\n",
    "        \n",
    "        llm = GoogleGenerativeAI(**config)\n",
    "        self.chain = prompt | llm\n",
    "\n",
    "    \n",
    "    def _extract_error_code(self, exception: Exception) -> Optional[int]:\n",
    "        try:\n",
    "            error_code = exception.code.value\n",
    "        except Exception:\n",
    "            error_code = None\n",
    "\n",
    "        return error_code\n",
    "        \n",
    "\n",
    "    def invoke(self, input: dict) -> str:\n",
    "        self._increment_counter(1)\n",
    "        try:\n",
    "            result = self.chain.invoke(input)\n",
    "        except Exception as exc:\n",
    "            if self._extract_error_code(exc) == 429:\n",
    "                print('Reaching maximum resources, wait to next minutes!')\n",
    "                self._wait_to_next_minute()\n",
    "                result = self.chain.invoke(input)\n",
    "\n",
    "            raise exc\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_llm_output(output: str):\n",
    "    \"\"\"\n",
    "    Parse the output of the LLM.\n",
    "    \n",
    "    The output of the LLM is expected to be in either '```python' or '```json' format.\n",
    "    This function will parse the output and return the result as a dictionary.\n",
    "    \n",
    "    Args:\n",
    "        output (str): The output of the LLM.\n",
    "    \n",
    "    Returns:\n",
    "        dict: The parsed output of the LLM.\n",
    "    \n",
    "    Raises:\n",
    "        Exception: If the output is not in the expected format.\n",
    "    \"\"\"\n",
    "    start = output.index('[')\n",
    "    end =  len(output) - output[::-1].index(']')\n",
    "\n",
    "    error_comma = end - 2 if output[end - 1] == ',' else end - 3\n",
    "    if output[error_comma] == ',':\n",
    "        output = output[:error_comma] + output[error_comma + 1:]\n",
    "\n",
    "    try:\n",
    "        res = json.loads(output[start:end])\n",
    "    except Exception:\n",
    "        try:\n",
    "            res = json.loads(output[start:end].lower())\n",
    "        except Exception:\n",
    "            raise Exception(f\"Could not parse output. Received: \\n{output}\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter by pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_filter(patterns: List[str], messages: List[dict], get_keyword: Optional[bool] = True) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Filter messages based on the presence or absence of specified keywords.\n",
    "\n",
    "    This function iterates through a list of messages and checks if each message contains any of the given keywords.\n",
    "    It returns a list of messages that either contain or do not contain the specified keywords, depending on the `get_keyword` flag.\n",
    "\n",
    "    Args:\n",
    "        patterns (List[str]): A list of keywords to filter by.\n",
    "        messages (List[dict]): A list of messages to filter.\n",
    "        get_keyword (Optional[bool], optional): If True, returns messages containing the keywords. \n",
    "            If False, returns messages not containing the keywords. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        List[dict]: A list of messages that meet the filtering criteria.\n",
    "    \"\"\"\n",
    "    synthetic_pattern = r'\\b(' + '|'.join(patterns) + r')\\b'\n",
    "    result = [m for m in messages \n",
    "              if bool(re.search(synthetic_pattern, m['message'].lower())) == get_keyword]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_template_message(templates: Dict[str, Dict[str, str]], messages: List[dict]) -> Tuple[List[dict], List[dict]]:\n",
    "    \"\"\"\n",
    "    Handle template messages.\n",
    "\n",
    "    This function iterates through a list of messages and checks if each message is a key in the `templates` dictionary.\n",
    "    If a message is found in the `templates` dictionary, it updates the message with the corresponding template information\n",
    "    and appends it to the `template_message` list. Otherwise, it appends the message to the `other_message` list.\n",
    "\n",
    "    Args:\n",
    "        templates (Dict[str, Dict[str, str]]): A dictionary of template messages, where the key is the message string\n",
    "            and the value is a dictionary containing the user and purpose information.\n",
    "        messages (List[dict]): A list of messages to be processed.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[dict], List[dict]]: A tuple containing two lists:\n",
    "            - `template_message`: A list of messages that were found in the `templates` dictionary.\n",
    "            - `other_message`: A list of messages that were not found in the `templates` dictionary.\n",
    "    \"\"\"\n",
    "    template_message = []\n",
    "    other_message = []\n",
    "    for m in messages:\n",
    "        key = m['message'].lower()\n",
    "        if key in templates:\n",
    "            m.update(templates[key])\n",
    "            template_message.append(m)\n",
    "        else:\n",
    "            other_message.append(m)\n",
    "\n",
    "    return template_message, other_message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(\n",
    "        messages: List[str], \n",
    "        prompt: ChatPromptTemplate,\n",
    "        batch_size: int = 50,\n",
    "        provider: Literal['groq', 'google'] = 'groq',\n",
    "        desc: Optional[str] = None,\n",
    ") -> List[Union[dict, bool, float, None]]:\n",
    "    if provider == \"google\":\n",
    "        chain = GoogleAICaller(LLM_CONFIG[provider], prompt)\n",
    "    else:\n",
    "        chain = GroqAICaller(LLM_CONFIG[provider], prompt)\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def cached_invoke(input_str):\n",
    "        return chain.invoke({\"input\": input_str})\n",
    "    \n",
    "    res = [None for _ in range(len(messages))]\n",
    "    with ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:\n",
    "        future = {\n",
    "            executor.submit(\n",
    "                lambda: cached_invoke(str(messages[i : i + batch_size]))\n",
    "            ): i\n",
    "            for i in range(0, len(messages), batch_size)\n",
    "        }\n",
    "        for f in tqdm(as_completed(future), total=len(future), desc=(desc or 'Loading')):\n",
    "            i = future[f]\n",
    "            end_idx = min(i + batch_size, len(messages))\n",
    "            \n",
    "            try:\n",
    "                response = f.result()\n",
    "            except Exception as exc:\n",
    "                print(f\"Error while generating response for batch {i} - {end_idx - 1}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                parsed_response = _parse_llm_output(response)\n",
    "                if len(parsed_response) != end_idx - i:\n",
    "                    print('Wrong size while query LLM, shutting down!')\n",
    "                    break\n",
    "\n",
    "                for j, idx in enumerate(range(i, end_idx)):\n",
    "                    res[idx] = next(iter(parsed_response[j].values()))\n",
    "            except Exception:\n",
    "                print(f\"Error while parsing LLM output for batch {i} - {end_idx - 1}\")\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_inquiry_pipeline(\n",
    "    messages: List[dict],\n",
    "    min_score: float,\n",
    "    batch_size: int = 50,\n",
    "    provider: Literal[\"google\", \"groq\"] = \"groq\",\n",
    ") -> Tuple[List[dict], List[dict]]:\n",
    "    input = [m[\"message\"] for m in messages]\n",
    "    output = call_llm(\n",
    "        messages=input,\n",
    "        prompt=classifying_inquiry_prompt,\n",
    "        batch_size=batch_size,\n",
    "        provider=provider,\n",
    "        desc='Classify inquiry'\n",
    "    )\n",
    "\n",
    "    classified_messages = []\n",
    "    error_messages = []\n",
    "    for message, score in zip(messages, output):\n",
    "        if score and score >= min_score:\n",
    "            classified_messages.append(message)\n",
    "        else:\n",
    "            error_messages.append(message)\n",
    "\n",
    "    return classified_messages, error_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reclassify_inquiry_pipeline(\n",
    "    messages: List[dict],\n",
    "    batch_size: int = 50,\n",
    "    provider: Literal[\"google\", \"groq\"] = \"groq\",\n",
    ") -> Tuple[List[dict], List[dict]]:\n",
    "    input = [m[\"message\"] for m in messages]\n",
    "    output = call_llm(\n",
    "        messages=input,\n",
    "        prompt=reclassifying_inquiry_prompt,\n",
    "        batch_size=batch_size,\n",
    "        provider=provider,\n",
    "        desc='Re-classify inquiry'\n",
    "    )\n",
    "\n",
    "    # get output to return\n",
    "    classified_messages = []\n",
    "    error_messages = []\n",
    "    for message, label in zip(messages, output):\n",
    "        if label:\n",
    "            classified_messages.append(message)\n",
    "        else:\n",
    "            error_messages.append(message)\n",
    "\n",
    "    return classified_messages, error_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_question_pipeline(\n",
    "    messages: List[dict],\n",
    "    batch_size: int = 50,\n",
    "    provider: Literal[\"google\", \"groq\"] = \"groq\",\n",
    ") -> Tuple[List[dict], List[dict]]:\n",
    "    # classify by LLM\n",
    "    input = [m[\"message\"] for m in messages]\n",
    "    output = call_llm(\n",
    "        messages=input,\n",
    "        prompt=classifying_important_question_prompt,\n",
    "        batch_size=batch_size,\n",
    "        provider=provider,\n",
    "        desc='Classify question'\n",
    "    )\n",
    "\n",
    "    # get output to return\n",
    "    classified_messages = []\n",
    "    error_messages = []\n",
    "    for message, label in zip(messages, output):\n",
    "        if label:\n",
    "            classified_messages.append(message)\n",
    "        else:\n",
    "            error_messages.append(message)\n",
    "\n",
    "    return classified_messages, error_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_user_purpose_pipeline(\n",
    "    messages: List, \n",
    "    batch_size: int = 50, \n",
    "    provider: Literal['google', 'groq'] = 'groq'\n",
    ") -> Tuple[List[dict], List[dict]]:\n",
    "    # classify by LLM\n",
    "    input = [m[\"message\"] for m in messages]\n",
    "    output = call_llm(\n",
    "        messages=input,\n",
    "        prompt=extracting_user_purpose_prompt,\n",
    "        batch_size=batch_size,\n",
    "        provider=provider,\n",
    "        desc='Extract inquiry'\n",
    "    )\n",
    "\n",
    "    extracted_messages = []\n",
    "    error_messages = []\n",
    "    for mess, u_and_p in zip(messages, output):\n",
    "        if u_and_p:\n",
    "            extracted_message = mess.copy()\n",
    "            extracted_message.update(u_and_p)\n",
    "            extracted_messages.append(extracted_message)\n",
    "        else:\n",
    "            error_messages.append(mess)\n",
    "        \n",
    "    return extracted_messages, error_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entire Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_message_pipeline(messages: List[dict],\n",
    "                             remove_keywords: List[str] = None,\n",
    "                             filter_keywords: List[str] = None,\n",
    "                             question_keywords: List[str] = None,\n",
    "                             template: Optional[dict] = None,\n",
    "                             important_score: Optional[float] = 0.7,\n",
    "                             batch_size: int = 50,\n",
    "                             provider: Literal['google', 'groq'] = 'groq'):\n",
    "    # Initialize results\n",
    "    extracted_messages = []\n",
    "    error_messages = []\n",
    "\n",
    "    # start processing\n",
    "    if remove_keywords:\n",
    "        messages = keyword_filter(remove_keywords, messages, get_keyword=False)\n",
    "\n",
    "    if filter_keywords:\n",
    "        messages = keyword_filter(filter_keywords, messages, get_keyword=True)\n",
    "\n",
    "    template_messages = None\n",
    "    if template is not None:\n",
    "        template_messages, messages = handle_template_message(template, messages)\n",
    "        extracted_messages += template_messages\n",
    "\n",
    "    # return messages\n",
    "    # extract user and purpose\n",
    "    classified_mess, error = classify_inquiry_pipeline(messages, important_score, batch_size, provider)\n",
    "    error_messages += error\n",
    "\n",
    "    extracted_mess, error = extract_user_purpose_pipeline(classified_mess, batch_size, provider)\n",
    "    extracted_messages += extracted_mess\n",
    "    error_messages += error\n",
    "\n",
    "    # classifiy important questions\n",
    "    if question_keywords is not None:\n",
    "        messages = keyword_filter(question_keywords, messages, get_keyword=True)\n",
    "\n",
    "    questions, error = classify_question_pipeline(messages, batch_size=batch_size, provider=provider)\n",
    "    error_messages += error\n",
    "\n",
    "    # deduplicate error messages\n",
    "    error_messages = [json.loads(item) for item in {json.dumps(d, sort_keys=True) for d in error_messages}]\n",
    "    \n",
    "    return extracted_messages, questions, error_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size after filtering: 421\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d91a5b07527436c9f1d0256f50af2d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classify inquiry:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 50 - 99\n",
      "batch 200 - 249\n",
      "batch 100 - 149\n",
      "batch 0 - 49\n",
      "batch 300 - 349\n",
      "Reaching maximum resources, wait to next minutes!\n",
      "Reaching maximum resources, wait to next minutes!\n",
      "batch 250 - 299\n",
      "batch 150 - 199\n",
      "batch 400 - 420\n",
      "Error while generating response for batch 400 - 420\n",
      "batch 350 - 399\n",
      "Error while generating response for batch 350 - 399\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2efcf48c86bc46c08205a2ff113ed35b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extract inquiry:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reaching maximum resources, wait to next minutes!\n",
      "Reaching maximum resources, wait to next minutes!\n",
      "batch 50 - 67\n",
      "batch 0 - 49\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4decad19349f401295047521e455b0fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classify question:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reaching maximum resources, wait to next minutes!\n",
      "Reaching maximum resources, wait to next minutes!\n",
      "batch 50 - 89\n",
      "batch 0 - 49\n"
     ]
    }
   ],
   "source": [
    "important_keywords = config['product-keywords'] + config['important-message-keywords']\n",
    "\n",
    "# extracted_messages = analyse_message_pipeline(\n",
    "extracted_messages, questions, error_messages = analyse_message_pipeline(\n",
    "    sample,\n",
    "    remove_keywords=config['unimportant-message-keywords'],\n",
    "    filter_keywords=config['important-message-keywords'] + config['product-keywords'],\n",
    "    question_keywords=config['question-keywords'],\n",
    "    template=config['template-message'],\n",
    "    important_score=config['important-score'],\n",
    "    provider='groq'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
