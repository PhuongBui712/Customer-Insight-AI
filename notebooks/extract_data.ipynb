{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(os.path.join(os.getcwd(), '../src'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from threading import Lock\n",
    "from typing import Dict\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "\n",
    "from prompt import inquiry_classifying_prompt, keyword_extracting_prompt\n",
    "from utils import *\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing Pipeline\n",
    "\n",
    "In this step, we will apply 2 methods to extract insightful data from customer's message:\n",
    "\n",
    "- **Meaningful inquiries**: Use LLM to detect any important, insightful customer's inquiries about products.\n",
    "- **Extracting keyword**: Use LLM to distil important keywords in messages\n",
    "\n",
    "We will combine these two methods into a complete pipeline to extract valuable information from customer messages. This pipeline will first classify messages as insightful inquiries, and then extract keywords from those classified messages. This approach allows us to focus on the most relevant information and gain deeper insights into customer needs and preferences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LOAD DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = load_json('../data/total_message.json')\n",
    "customer_messages = [m for m in messages if m['from'] == 'customer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "START = 0\n",
    "N = 1000\n",
    "\n",
    "sample = customer_messages[START : START + N]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LOAD LLM**\n",
    "\n",
    "We will use ***Gemini-1.5-flash*** of Google, which is one of the state of the art LLMs (or even Multimodal model) in the present. Furthermore, this model is also provided a good API capacity for free tier.\n",
    "\n",
    "Because of requirement of precision and static output, we also need to modify `temperature`, `top_p`, and `top_k` to ensure model work accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_yaml('../config.yaml')\n",
    "LLM_CONFIG = config['llm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMCaller:\n",
    "    \"\"\"\n",
    "    A class to manage the rate of requests to an LLM.\n",
    "    \n",
    "    This class implements a simple rate limiting mechanism to prevent exceeding the maximum number of requests per minute allowed by the LLM API.\n",
    "    \n",
    "    Attributes:\n",
    "        max_request_per_minute (int): The maximum number of requests allowed per minute.\n",
    "        _request_counter (int): The number of requests made in the current minute.\n",
    "        _last_reset_time (float): The timestamp of the last time the request counter was reset.\n",
    "        _state_lock (Lock): A lock to protect the request counter and last reset time from concurrent access.\n",
    "    \"\"\"\n",
    "    _request_counter = 0\n",
    "    _last_reset_time = 0.0\n",
    "    _state_lock = Lock()\n",
    "\n",
    "    def __init__(self, max_request_per_minute: int):\n",
    "        self.max_request_per_minute = max_request_per_minute\n",
    "\n",
    "    def _reset_counter(self):\n",
    "        current_time = time.time()\n",
    "        if self._last_reset_time == 0.0 or current_time - self._last_reset_time >= 60:\n",
    "            self._request_counter = 0\n",
    "            self._last_reset_time = current_time\n",
    "\n",
    "\n",
    "    def _increment_counter(self, num_request):\n",
    "        with self._state_lock:\n",
    "            self._reset_counter()\n",
    "            if self._request_counter + num_request > self.max_request_per_minute:\n",
    "                time.sleep(max(0, self._last_reset_time + 60 - time.time()))\n",
    "                self._reset_counter()\n",
    "            self._request_counter += num_request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroqAICaller(LLMCaller):\n",
    "    def __init__(self, llm_config: dict, prompt: ChatPromptTemplate):\n",
    "        super().__init__(max_request_per_minute=30)\n",
    "\n",
    "        llm = ChatGroq(**llm_config)\n",
    "        self.chain = prompt | llm\n",
    "\n",
    "    def invoke(self, input: dict):\n",
    "        self._increment_counter(1)\n",
    "\n",
    "        return self.chain.invoke(input).content\n",
    "\n",
    "\n",
    "class GoogleAICaller(LLMCaller):\n",
    "    def __init__(self, llm_config: dict, prompt: PromptTemplate):\n",
    "        super().__init__(max_request_per_minute=15)\n",
    "\n",
    "        llm = GoogleGenerativeAI(**llm_config)\n",
    "        self.chain = prompt | llm\n",
    "\n",
    "\n",
    "    def invoke(self, input: dict):\n",
    "        self._increment_counter(1)\n",
    "\n",
    "        result = self.chain.invoke(input)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_llm_output(output: str):\n",
    "    \"\"\"\n",
    "    Parse the output of the LLM.\n",
    "    \n",
    "    The output of the LLM is expected to be in either '```python' or '```json' format.\n",
    "    This function will parse the output and return the result as a dictionary.\n",
    "    \n",
    "    Args:\n",
    "        output (str): The output of the LLM.\n",
    "    \n",
    "    Returns:\n",
    "        dict: The parsed output of the LLM.\n",
    "    \n",
    "    Raises:\n",
    "        Exception: If the output is not in the expected format.\n",
    "    \"\"\"\n",
    "    start = output.index('[')\n",
    "    end =  len(output) - output[::-1].index(']')\n",
    "\n",
    "    error_comma = end - 2 if output[end - 1] == ',' else end - 3\n",
    "    if output[error_comma] == ',':\n",
    "        output = output[:error_comma] + output[error_comma + 1:]\n",
    "\n",
    "    try:\n",
    "        res = json.loads(output[start:end])\n",
    "    except Exception:\n",
    "        try:\n",
    "            res = json.loads(output[start:end].lower())\n",
    "        except Exception:\n",
    "            raise Exception(f\"Could not parse output. Received: \\n{output}\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Inquiries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_filter_message(patterns: List[dict], messages: List[str]) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Filter messages that contain any of the given keywords.\n",
    "    \n",
    "    Args:\n",
    "        patterns (List[dict]): A list of keywords to filter.\n",
    "        messages (List[str]): A list of messages to filter.\n",
    "    \n",
    "    Returns:\n",
    "        List[dict]: A list of messages that do not contain any of the given keywords.\n",
    "    \"\"\"\n",
    "    synthetic_pattern = r'\\b(' + '|'.join(patterns) + r')\\b'\n",
    "    result = [m for m in messages if not re.search(synthetic_pattern, m['message'])]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_inquiry_pipeline(messages: List[dict],\n",
    "                              min_score: float,\n",
    "                              batch_size: int = 50,\n",
    "                              provider: Literal['google', 'groq'] = 'groq') -> Tuple[List[dict], List[dict]]:\n",
    "    # classify by LLM\n",
    "    if provider == 'google':\n",
    "        chain = GoogleAICaller(LLM_CONFIG[provider], inquiry_classifying_prompt)\n",
    "    else:\n",
    "        chain = GroqAICaller(LLM_CONFIG[provider], inquiry_classifying_prompt)\n",
    "\n",
    "    mask = []\n",
    "    for i in tqdm(range(0, len(messages), batch_size), desc='Detecting insightful inquiry'):\n",
    "        end_idx = min(len(messages), i + batch_size)\n",
    "        try:\n",
    "            response = chain.invoke({'input': str(messages[i : end_idx])})\n",
    "        except Exception:\n",
    "            print(f'Error while generating response for batch {i} - {end_idx}')\n",
    "            \n",
    "            mask += ['error' for _ in range(i, end_idx)]\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            parsed_response = _parse_llm_output(response)\n",
    "            mask += [list(r.items())[0][1] for r in parsed_response]\n",
    "        except Exception:\n",
    "            print(f'Error while parsing LLM output for batch {i} - {end_idx}')\n",
    "\n",
    "            mask += ['error' for _ in range(i, end_idx)]\n",
    "    \n",
    "    # get output to return\n",
    "    classified_messages = [m for m, l in zip(messages, mask) if l >= min_score]\n",
    "    error_messages = [m for m, l in zip(messages, mask) if l == 'error']\n",
    "\n",
    "    return classified_messages, error_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_template_message(templates: Dict[str, Dict[str, str]], messages: List[dict]) -> Tuple[List[dict], List[dict]]:\n",
    "    \"\"\"\n",
    "    Handle template messages.\n",
    "\n",
    "    This function iterates through a list of messages and checks if each message is a key in the `templates` dictionary.\n",
    "    If a message is found in the `templates` dictionary, it updates the message with the corresponding template information\n",
    "    and appends it to the `template_message` list. Otherwise, it appends the message to the `other_message` list.\n",
    "\n",
    "    Args:\n",
    "        templates (Dict[str, Dict[str, str]]): A dictionary of template messages, where the key is the message string\n",
    "            and the value is a dictionary containing the user and purpose information.\n",
    "        messages (List[dict]): A list of messages to be processed.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[dict], List[dict]]: A tuple containing two lists:\n",
    "            - `template_message`: A list of messages that were found in the `templates` dictionary.\n",
    "            - `other_message`: A list of messages that were not found in the `templates` dictionary.\n",
    "    \"\"\"\n",
    "    template_message = []\n",
    "    other_message = []\n",
    "    for m in messages:\n",
    "        key = m['message'].lower()\n",
    "        if key in templates:\n",
    "            m.update(templates[key])\n",
    "            template_message.append(m)\n",
    "        else:\n",
    "            other_message.append(m)\n",
    "\n",
    "    return template_message, other_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keyword_pipeline(messages: List, \n",
    "                             batch_size: int = 50, \n",
    "                             provider: Literal['google', 'groq'] = 'groq') -> Tuple[List[dict], List[dict]]:\n",
    "    if provider == 'google':\n",
    "        chain = GoogleAICaller(LLM_CONFIG[provider], keyword_extracting_prompt)\n",
    "    else:\n",
    "        chain = GroqAICaller(LLM_CONFIG[provider], keyword_extracting_prompt)\n",
    "    \n",
    "    keywords = []\n",
    "    for i in tqdm(range(0, len(messages), batch_size), desc='Extracting keywords'):\n",
    "        end_idx = min(len(messages), i + batch_size)\n",
    "        try:\n",
    "            response = chain.invoke({'input': str(messages[i : end_idx])})\n",
    "        \n",
    "        except Exception:\n",
    "            print(f'Error while generating response for batch {i} - {end_idx}')\n",
    "            keywords += ['error' for _ in range(i, end_idx)]\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            parsed_response = _parse_llm_output(response)\n",
    "            keywords += parsed_response\n",
    "\n",
    "        except Exception as exc:\n",
    "            print(f'Error while parsing LLM output for batch {i} - {end_idx}: {exc}')\n",
    "            keywords += ['error' for _ in range(i, end_idx)]\n",
    "\n",
    "    extracted_messages = []\n",
    "    error_messages = []\n",
    "    for mess, kw_item in zip(messages, keywords):\n",
    "        if kw_item != 'error':\n",
    "            k, v = list(kw_item.items())[0]\n",
    "            if all(len(x) > 0 for x in v.values()):\n",
    "                extracted_messages.append(mess.copy())\n",
    "                extracted_messages[-1].update(v)\n",
    "        else:\n",
    "            error_messages.append(mess.copy())\n",
    "\n",
    "    return extracted_messages, error_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entire Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_message_pipeline(messages: List[dict],\n",
    "                             filter_patterns: Optional[List[str]] = None, \n",
    "                             template_messages: Optional[dict] = None,\n",
    "                             important_score: Optional[float] = 0.7,\n",
    "                             batch_size: int = 50,\n",
    "                             provider: Literal['google', 'groq'] = 'groq'):\n",
    "    # Initialize results\n",
    "    processed_messages = []\n",
    "    error_messages = []\n",
    "\n",
    "    # start processing\n",
    "    if filter_patterns is not None:\n",
    "        messages = keyword_filter_message(filter_patterns, messages)\n",
    "\n",
    "    if template_messages is not None:\n",
    "        template, messages = handle_template_message(template_messages, messages)\n",
    "        processed_messages += template\n",
    "\n",
    "    messages, error = classify_inquiry_pipeline(messages, important_score, batch_size, provider)\n",
    "    error_messages += error\n",
    "\n",
    "    messages, error = extract_keyword_pipeline(messages, batch_size, provider)\n",
    "    error_messages += error\n",
    "    processed_messages += messages\n",
    "\n",
    "    return processed_messages, error_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "969913b5e2954c4c8276e438b2bebee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Detecting insightful inquiry:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28db1133816b455c946ca785fecddb0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting keywords:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = processed_messages, error_messages = analyse_message_pipeline(\n",
    "    sample,\n",
    "    config['filter-message-keywords'],\n",
    "    config['template-message'],\n",
    "    important_score=config['important-score']\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
